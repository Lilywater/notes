* memory usage
** free command
[root@allinone ~]# free -mh
             total        used        free      shared  buff/cache   available
Mem:         251G        184G        812M        4.0G         66G         62G

 free  displays  the  total  amount of free and used physical and swap memory in the system, as well as the buffers and caches used by the kernel. The
 information is gathered by parsing /proc/meminfo. The displayed columns are:

       total  Total installed memory (MemTotal and SwapTotal in /proc/meminfo)

       used   Used memory (calculated as total - free - buffers - cache)

       free   Unused memory (MemFree and SwapFree in /proc/meminfo)

       shared Memory used (mostly) by tmpfs (Shmem in /proc/meminfo, available on kernels 2.6.32, displayed as zero if not available)

       buffers
              Memory used by kernel buffers (Buffers in /proc/meminfo)

       cache  Memory used by the page cache and slabs (Cached and SReclaimable in /proc/meminfo)

       buff/cache
              Sum of buffers and cache

       available
              Estimation of how much memory is available for starting new applications, without swapping. Unlike the data provided  by  the  cache  or  free
              fields,  this  field  takes into account page cache and also that not all reclaimable memory slabs will be reclaimed due to items being in use
              (MemAvailable in /proc/meminfo, available on kernels 3.14, emulated on kernels 2.6.27+, otherwise the same as free)

here free is meaningless in user point of view, we care used and available


** top 
*** top free field
  Linux Memory Types
       For our purposes there are three types of memory, and one is optional.  First is physical memory, a limited resource where code and data must  reside
       when  executed or referenced.  Next is the optional swap file, where modified (dirty) memory can be saved and later retrieved if too many demands are
       made on physical memory.  Lastly we have virtual memory, a nearly unlimited resource serving the following goals:

          1. abstraction, free from physical memory addresses/limits
          2. isolation, every process in a separate address space
          3. sharing, a single mapping can serve multiple needs
          4. flexibility, assign a virtual address to a file

       Regardless of which of these forms memory may take, all are managed as pages  (typically  4096  bytes)  but  expressed  by  default  in  top  as  KiB
       (kibibyte).   The memory discussed under topic `2c. MEMORY Usage' deals with physical memory and the swap file for the system as a whole.  The memory
       reviewed in topic `3. FIELDS / Columns Display' embraces all three memory types, but for individual processes.

       For each such process, every memory page is restricted to a single quadrant from the table below.   Both  physical  memory  and  virtual  memory  can
       include  any  of the four, while the swap file only includes #1 through #3.  The memory in quadrant #4, when modified, acts as its own dedicated swap
       file.

                                     Private | Shared
                                 1           |          2
            Anonymous  . stack               |
                       . malloc()            |
                       . brk()/sbrk()        | . POSIX shm*
                       . mmap(PRIVATE, ANON) | . mmap(SHARED, ANON)
                      -----------------------+----------------------
                       . mmap(PRIVATE, fd)   | . mmap(SHARED, fd)
          File-backed  . pgms/shared libs    |
                                 3           |          4

       The following may help in interpreting process level memory values displayed as scalable columns and  discussed  under  topic  `3a.  DESCRIPTIONS  of
       Fields'.

          %MEM - simply RES divided by total physical memory
          CODE - the `pgms' portion of quadrant 3
          DATA - the entire quadrant 1 portion of VIRT plus all
                 explicit mmap file-backed pages of quadrant 3
          RES  - anything occupying physical memory which, beginning with
                 Linux-4.5, is the sum of the following three fields:
                 RSan - quadrant 1 pages, which include any
                        former quadrant 3 pages if modified
                 RSfd - quadrant 3 and quadrant 4 pages
                 RSsh - quadrant 2 pages
          RSlk - subset of RES which cannot be swapped out (any quadrant)
          SHR  - subset of RES (excludes 1, includes all 2 & 4, some 3)
          SWAP - potentially any quadrant except 4
          USED - simply the sum of RES and SWAP
          VIRT - everything in-use and/or reserved (all quadrants)

       Note: Even though program images and shared libraries are considered private to a process, they will be accounted for as shared (SHR) by the kernel.


*** show which core the process run on


This can be done with top command. The default top command output does not show these details.
 To view this detail you will have to press f key while on top command interface and then press j(press Enter key after you pressed j).
 Now the output will show you details regarding a process and which processor its running:
-------------------------------------------------------------------------
top - 04:24:03 up 96 days, 13:41,  1 user,  load average: 0.11, 0.14, 0.15
Tasks: 173 total,   1 running, 172 sleeping,   0 stopped,   0 zombie
Cpu(s):  7.1%us,  0.2%sy,  0.0%ni, 88.4%id,  0.1%wa,  0.0%hi,  0.0%si,  4.2%st
Mem:   1011048k total,   950984k used,    60064k free,     9320k buffers
Swap:   524284k total,   113160k used,   411124k free,    96420k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  P COMMAND
12426 nginx     20   0  345m  47m  29m S 77.6  4.8  40:24.92 7 php-fpm
 6685 mysql     20   0 3633m  34m 2932 S  4.3  3.5  63:12.91 4 mysqld
19014 root      20   0 15084 1188  856 R  1.3  0.1   0:01.20 4 top
    9 root      20   0     0    0    0 S  1.0  0.0 129:42.53 1 rcu_sched
 6349 memcache  20   0  355m  12m  224 S  0.3  1.2   9:34.82 6 memcached
    1 root      20   0 19404  212   36 S  0.0  0.0   0:20.64 3 init
    2 root      20   0     0    0    0 S  0.0  0.0   0:30.02 4 kthreadd
    3 root      20   0     0    0    0 S  0.0  0.0   0:12.45 0 ksoftirqd/0
---------------------------------------------------------------------------


* cpu usage
** docker cpu usage limitation
docker could specify various parameter realted to cpu usage


A program’s CPU usage is highly quantized and consumed in many very short bursts “to 100%” each second. This means the tactics for limiting usage can be different than memory usage.
 Server applications may allocate memory statically on startup, build in-memory caches, and consume memory in other ways that aren’t directly related to processing the workload.

Docker exposes two main Linux cpu usage controls to you via the container’s cgroup:
    cpu shares  
    cpu quota

CPU shares can help you establish a relative priority between containerized processes sharing a host. Also, because CPU shares was available before the absolute CPU constraint option, most container orchestrators know how to work with cpu shares expressed in ‘millicores’. These orchestrators will take a request or limit expressed in millicores, look for a container host with sufficient resources under the assumption that each core is worth 1024 millicores, and then configure the cpu-shares for the container accordingly. This approximates the ability to specify a certain number of cpus well for many use cases.
One of the places CPU shares breaks down is when you don’t want a container to be able to burst above its share when there is no contention on the host.

 There are a number of valid use cases such as:
    preventing a load testing of an isolated service from using a whole machine in a test environment when the service will only have part of a machine in production
    establishing a hard budget that limits the resources available to a service for scheduling, security, billing, or accounting reasons


*** CPU shares
The cpu-shares option allows you to specify the relative share of cpu a container will receive when there is contention for cpu.
When there is no contention for cpu, a container will get to use however much it wants, no matter what the ‘limit’ is.
When there is contention, a container configured for 2048 shares of a cpu will get twice as much cpu time as a container that requested 1024 cpu shares.

Let’s work through an example. I am using a Docker on a machine with 4 cores.
docker container run --name stresser-1024  --cpu-shares 1024  dockerinaction/ch6_stresser


Start by launching docker stats in one terminal, showing just the container name, cpu usage, and memory usage:
docker stats --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"

The Docker stats command shows that stresser-1024 container uses 398% cpu — all four cores:
NAME                CPU %               MEM USAGE / LIMIT     MEM %
stresser-1024       398.29%             1.102MiB / 1.945GiB   0.06%

Now let’s see what happens when running a second container with twice as many cpu shares:

docker container run -d --name stresser-2048  --cpu-shares 2048  dockerinaction/ch6_stresser
The Docker stats command shows that stresser-1024 container uses 398% cpu — all four cores:
NAME                CPU %               MEM USAGE / LIMIT     MEM %
stresser-1024       398.29%             1.102MiB / 1.945GiB   0.06%



running both stresser-1024 stresser-2048 containers

Now that there are two processes that both want to take 100% of CPU resources, Linux is dividing those resources between them proportionally:

NAME                CPU %               MEM USAGE / LIMIT     MEM %
stresser-2048       263.26%             1.078MiB / 1.945GiB   0.05%
stresser-1024       131.42%             1.035MiB / 1.945GiB   0.05% 
And indeed, stresser-2048 gets twice (263%) as much cpu as stresser-1024 (131%).

Notice that while we specified the cpu shares with what looks like an implied number of desired processors multiplied by 1024, this didn’t limit the processes to one or two cores. All four cores on the machine are used.

root@node6 ~]# docker inspect stresser-1024 |grep -i cpu
            "CpuShares": 1024,
            "NanoCpus": 0,
            "CpuPeriod": 0,
            "CpuQuota": 0,
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "",
            "CpusetMems": "",
            "CpuCount": 0,
            "CpuPercent": 0,

*** CPU quota
**** --cpus parameter
Docker permits you to configure absolute cpu quotas easily through the --cpus option introduced in Docker 1.13. This cpu quota specifies the fixed share of cpu that the container is entitled may use before it is throttled. The quota is defined at the container’s cgroup and enforced by Linux’s Completely Fair Scheduler.

Let’s see it in action by starting stresser containers with quotas of 1 and 2 cpus each:

docker container run -d --name stresser-1-cpus --cpus 1 dockerinaction/ch6_stresser
docker container run -d --name stresser-2-cpus --cpus 2 dockerinaction/ch6_stresser

Now the stats show a much different story:
NAME                CPU %               MEM USAGE / LIMIT     MEM %
stresser-1-cpus     100.17%             1.098MiB / 1.945GiB   0.06%
stresser-2-cpus     201.14%             1.07MiB / 1.945GiB    0.05%

The stresser programs are limited to precisely the number of cpus we specified. When a program exhausts its quota, the Linux kernel will delay running the program until the quota is replenished. The quota is allocated and enforced every 100 milliseconds.
Applying CPU constraints

[root@node6 ~]# docker inspect stresser_cpu1 |grep -i cpu
        "Name": "/stresser_cpus1",
            "CpuShares": 0,
            "NanoCpus": 1000000000,
            "CpuPeriod": 0,
            "CpuQuota": 0,
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "",
            "CpusetMems": "",
            "CpuCount": 0,
            "CpuPercent": 0,


****  --cpu-period and --cpu-quota

docker run  --cpu-period=100000 --cpu-quota=200000

[root@node6 ~]# docker inspect stresser_cpuquota2 |grep -i cpu
        "Name": "/stresser_cpuquota2",
            "CpuShares": 2048,
            "NanoCpus": 0,
            "CpuPeriod": 100000,
            "CpuQuota": 200000,
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "",
            "CpusetMems": "",
            "CpuCount": 0,
            "CpuPercent": 0,

** docker cpu options
*** option--cpus=<value>	
Specify how much of the available CPU resources a container can use. For instance, if the host machine has two CPUs and you set --cpus="1.5", the container is guaranteed at most one and a half of the CPUs. This is the equivalent of setting --cpu-period="100000" and --cpu-quota="150000".

*** option  --cpu-period=<value>	
Specify the CPU CFS scheduler period, which is used alongside --cpu-quota. Defaults to 100000 microseconds (100 milliseconds). Most users do not change this from the default. For most use-cases, --cpus is a more convenient alternative.
*** option --cpu-quota=<value>	
Impose a CPU CFS quota on the container. The number of microseconds per --cpu-period that the container is limited to before throttled. As such acting as the effective ceiling. For most use-cases, --cpus is a more convenient alternative.

*** option --cpuset-cpus	
Limit the specific CPUs or cores a container can use. A comma-separated list or hyphen-separated range of CPUs a container can use, if you have more than one CPU.
 The first CPU is numbered 0. A valid value might be 0-3 (to use the first, second, third, and fourth CPU) or 1,3 (to use the second and fourth CPU).

*** option --cpu-shares 	
Set this flag to a value greater or less than the default of 1024 to increase or reduce the container’s weight, and give it access to a greater or lesser proportion of the 
host machine’s CPU cycles. This is only enforced when CPU cycles are constrained. When plenty of CPU cycles are available, all containers use as much CPU as they need.
 In that way, this is a soft limit. --cpu-shares does not prevent containers from being scheduled in swarm mode. It prioritizes container CPU resources for the available CPU cycles.
 It does not guarantee or reserve any specific CPU access.

If you have 1 CPU, each of the following commands guarantees the container at most 50% of the CPU every second.
docker run -it --cpus=".5" ubuntu /bin/bash

** kubernetes cpu resource VS. docker cpu resources
*** kubelet service configuration file
cat sudo cat  /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml"
# This is a file that "kubeadm init" and "kubeadm join" generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
EnvironmentFile=-/etc/default/kubelet
ExecStart=
Environment="KUBELET_CPUMANAGER=--cpu-manager-policy=static --reserved-cpus=0,4,8,12"

worker nodes's cpu allocation policy 
=--cpu-manager-policy=static --reserved-cpus=0,4,8,12

ubuntu@master:~/deployment$ sudo cat  /var/lib/kubelet/cpu_manager_state
{"policyName":"none","defaultCpuSet":"","checksum":1353318690}

master$ ssh -t node14  sudo cat  /var/lib/kubelet/cpu_manager_state
{"policyName":"static","defaultCpuSet":"0,4,6-8,12,14-15","entries":{"931692b6-91de-4258-8b0f-ec634ad86bc0":{"ipds":"3,5,11,13"},"c9fb6314-f8fa-4beb-b9bd-4d7de4741b5f":{"amms":"1-2,9-10"}},"checksum":3853204946}Connection to 10.69.151.44 closed.


*** helm settings
---------
  loglocal:
      imageName: 192.168.26.10:5000/cmm-loglocal:CMM21.8.0_B1_C1630
      resources:
        cpu: 0.1
        memory: 96Mi
-------------
    emms_amms:
      imageName: 192.168.26.10:5000/cmm-aimcpps:CMM21.8.0_B1_C1630
      resources:
        cpu: 2
        memory: 17Gi
----------------------------------------
-
*** kubernet describe pods settings
loglocal:
------------------------------------
   Limits:
      cpu:     100m   ### 100m is based on 1024 as cpuShares, 1024*0.1
      memory:  96Mi
    Requests:
      cpu:     100m
      memory:  96Mi
------------------------------

amms:
-------------------
   Limits:
      cpu:     4
      memory:  17Gi
    Requests:
      cpu:      4
      memory:   17Gi
------------------------

*** docker settings
check loglocal's docker settings
de14:~$ sudo docker inspect 917caf2be70f |grep -i cpu
            "CpuShares": 102,  ### default is 1024, 0.1*1024= 102
            "NanoCpus": 0,
            "CpuPeriod": 100000,
            "CpuQuota": 10000,  ### only 1 cpuPeriod 1*CpuPeriod
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "0,4,6-8,12,14-15",  ### it can run on any cpu core except for the reservations
            "CpusetMems": "",
            "CpuCount": 0,
            "CpuPercent": 0,
                "CPU_LIMIT=1",

check amms's docker settings
ubuntu@node14:~$ sudo docker inspect dc6c329bfc71 |grep -i cpu
            "CpuShares": 4096,  #### deafault is 1024, 4*1024 = 4096
            "NanoCpus": 0,
            "CpuPeriod": 100000,
            "CpuQuota": 400000,  ### 4*CpuPeriod
            "CpuRealtimePeriod": 0,
            "CpuRealtimeRuntime": 0,
            "CpusetCpus": "1-2,9-10",
            "CpusetMems": "",
            "CpuCount": 0,
            "CpuPercent": 0,
                "CPU_LIMIT=4",
ubuntu@node14:~$

*** docker memory limitations
loglocal memory:
ubuntu@node14:~$ sudo docker inspect 917caf2be70f |grep -i mem
            "Memory": 100663296,   ### 96M
            "CpusetMems": "",
            "KernelMemory": 0,
            "KernelMemoryTCP": 0,
            "MemoryReservation": 0,
            "MemorySwap": -1,
            "MemorySwappiness": null,
                "MEMORY_LIMIT=100663296",

-----------------------------------
amms memory:
ubuntu@node14:~$ sudo docker inspect dc6c329bfc71 |grep -i mem
            "Memory": 18253611008,    ### 17G
            "CpusetMems": "",
            "KernelMemory": 0,
            "KernelMemoryTCP": 0,
            "MemoryReservation": 0,
            "MemorySwap": -1,
            "MemorySwappiness": null,
                "MEMORY_LIMIT=18253611008",



* memory limitaions


** memory limitaion with --memory and --memory-swap
*** option --memory
-m or --memory= The maximum amount of memory the container can use. If you set this option, the minimum allowed value is 4m (4 megabyte).
this is the RAM memory value


*** option --memory-swap
The amount of memory this container is allowed to swap to disk. See --memory-swap details.
if we set --memory as a limitation, the --memory-swap will be as twice as --memory parameter.

*** example of only --memory
docker run --memory 50m --rm -it progrium/stress --vm 1 --vm-bytes 62914560 --timeout 1s
[root@node6 ~]# docker run  --memory 70m --rm  -it progrium/stress  --help
`stress' imposes certain types of compute stress on your system

Usage: stress [OPTION [ARG]] ...
 -?, --help         show this help statement
     --version      show version statement
 -v, --verbose      be verbose
 -q, --quiet        be quiet
 -n, --dry-run      show what would have been done
 -t, --timeout N    timeout after N seconds
     --backoff N    wait factor of N microseconds before work starts
 -c, --cpu N        spawn N workers spinning on sqrt()
 -i, --io N         spawn N workers spinning on sync()
 -m, --vm N         spawn N workers spinning on malloc()/free()
     --vm-bytes B   malloc B bytes per vm worker (default is 256MB)
     --vm-stride B  touch a byte every B bytes (default is 4096)
     --vm-hang N    sleep N secs before free (default is none, 0 is inf)
     --vm-keep      redirty memory instead of freeing and reallocating
 -d, --hdd N        spawn N workers spinning on write()/unlink()
     --hdd-bytes B  write B bytes per hdd worker (default is 1GB)
     --hdd-noclean  do not unlink files created by hdd workers

Example: stress --cpu 8 --io 4 --vm 2 --vm-bytes 128M --timeout 10s

Note: Numbers may be suffixed with s,m,h,d,y (time) or B,K,M,G (size).


[root@node6 ~]# docker inspect 7fb628ce2c7a |grep -i mem
            "Memory": 52428800,     #### the --memory 50m parameter
            "CpusetMems": "",
            "KernelMemory": 0,
            "KernelMemoryTCP": 0,
            "MemoryReservation": 0,
            "MemorySwap": 104857600,  ### if no --memory-swap set but --memory is set, the value is twice of that 
            "MemorySwappiness": null,
argument is the parameter of porgram run inside container's
"Args": [ "--verbose", "--vm", "1", "--vm-bytes", "62914560", "--timeout", "1s" ],

*** example of both --memory --memory-swap
docker run --memory 50m --memory-swap 50m   -it progrium/stress --vm 1 --vm-bytes 62914560 --timeout 1s

[root@node6 ~]# docker inspect 7fb628ce2c7a |grep -i mem
            "Memory": 52428800,     #### the --memory 50m parameter
            "CpusetMems": "",
            "KernelMemory": 0,
            "KernelMemoryTCP": 0,
            "MemoryReservation": 0,
            "MemorySwap": 52428800,  ### if no --memory-swap  parameter
            "MemorySwappiness": null,

*** --oom-kill-disable	
By default, if an out-of-memory (OOM) error occurs, the kernel kills processes in a container. To change this behavior, use the --oom-kill-disable option. Only disable the OOM killer on containers where you have also set the -m/--memory option. If the -m flag is not set, the host can run out of memory and the kernel may need to kill the host system’s processes to free memor

the container will failed in OOMKILLED
[root@node6 ~]# docker inspect 7c6daa4aaa75 |grep -i oom
            "OOMKilled": true,
            "OomKillDisable": false,


*** memory swappiness rate
--memory-swappiness	By default, the host kernel can swap out a percentage of anonymous pages used by a container. You can
 set --memory-swappiness to a value between 0 and 100, to tune this percentage. See --memory-swappiness details.

*** memory checking
docker run  --memory 70m --memory-swap 70m  --rm  -it progrium/stress  --vm 1 --vm-bytes 62914560 --timeout 30s

docker stats
CONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT     MEM %     NET I/O     BLOCK I/O   PIDS
e470e6314d15   registry         0.11%     9.043MiB / 94.24GiB   0.01%     656B / 0B   0B / 0B     34
4dfb35299cd7   beautiful_bohr   99.94%    51.26MiB / 70MiB      73.23%    516B / 0B   0B / 0B     2

if we get 62m of memory alloction, but limitation on 70m, we can see that memory usage is 73%


** other memory limitations
--memory-reservation	Allows you to specify a soft limit smaller than --memory which is activated when Docker detects contention 
or low memory on the host machine. If you use --memory-reservation, it must be set lower than --memory for it to take precedence.
 Because it is a soft limit, it does not guarantee that the container doesn’t exceed the limit.
