* upload data 


** create a database in console
*** create an index
PUT employees


*** create mapping of this index
PUT employees/_mapping
{ "properties": { "date_of_birth": { "type": "date", "format": "dd/MM/yyyy" } } }


*** upload many recoreds to the index using post _bulk
POST _bulk
{ "index" : { "_index" : "employees", "_id" : "1" } }
{"id":1,"name":"Huntlee Dargavel","email":"hdargavel0@japanpost.jp","gender":"male","ip_address":"58.11.89.193","date_of_birth":"11/09/1990","company":"Talane","position":"Research Associate","experience":7,"country":"China","phrase":"Multi-channelled coherent leverage","salary":180025}
{ "index" : { "_index" : "employees", "_id" : "2" } }
{"id":2,"name":"Othilia Cathel","email":"ocathel1@senate.gov","gender":"female","ip_address":"3.164.153.228","date_of_birth":"22/07/1987","company":"Edgepulse","position":"Structural Engineer","experience":11,"country":"China","phrase":"Grass-roots heuristic help-desk","salary":193530}
{ "index" : { "_index" : "employees", "_id" : "3" } }
{"id":3,"name":"Winston Waren","email":"wwaren2@4shared.com","gender":"male","ip_address":"202.37.210.94","date_of_birth":"10/11/1985","company":"Yozio","position":"Human Resources Manager","experience":12,"country":"China","phrase":"Versatile object-oriented emulation","salary":50616}
{ "index" : { "_index" : "employees", "_id" : "4" } }
{"id" : 4,"name" : "Alan Thomas","email" : "athomas2@example.com","gender" : "male","ip_address" : "200.47.210.95","date_of_birth" : "11/12/1985","company" : "Yamaha","position" : "Resources Manager","experience" : 12,"country" : "China","phrase" : "Emulation of roots heuristic coherent systems","salary" : 300000}

** update a record with _doc id
update one record with record id 5
PUT employees/_doc/5
{
  "id": 5,
  "name": "Michael Bordon",
  "email": "mbordon@example.com",
  "gender": "male",
  "ip_address": "10.47.210.65",
  "date_of_birth": "12/12/1995",
  "position": "Resources Manager",
  "experience": 12,
  "country": "USA",
  "phrase": "Emulation of roots heuristic coherent systems",
  "company": "google",
  "salary": 300000
}
** get info from a database
GET employees
---
employees" : {
    "aliases" : { },
    "mappings" : {
      "properties" : {
        "company" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "country" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "date_of_birth" : {
          "type" : "date",
          "format" : "dd/MM/yyyy"
        },
--------------
the employee database mapping fields will be created automatically by the real data.


GET employees/_search
{
    "query" : {
        "match_all" : {}
    }
}
---------
get all all records from the database


*** sql format query and result
POST /_sql/?format=txt
{ "query": "SELECT * FROM employees" }
--------------------------------------------------------------------------------------------------------------------------------------------------------
    company    |    country    |     date_of_birth      |         email         |  experience   |    gender     |      id       |  ip_address   |      name      |                   phrase                    |       position        |    salary     
---------------+---------------+------------------------+-----------------------+---------------+---------------+---------------+---------------+----------------+---------------------------------------------+-----------------------+---------------
Talane         |China          |1990-09-11T00:00:00.000Z|hdargavel0@japanpost.jp|7              |male           |1              |58.11.89.193   |Huntlee Dargavel|Multi-channelled coherent leverage           |Research Associate     |180025         
Edgepulse      |China          |1987-07-22T00:00:00.000Z|ocathel1@senate.gov    |11             |female         |2              |3.164.153.228  |Othilia Cathel  |Grass-roots heuristic help-desk              |Structural Engineer    |193530         
Yozio          |China          |1985-11-10T00:00:00.000Z|wwaren2@4shared.com    |12             |male           |3              |202.37.210.94  |Winston Waren   |Versatile object-oriented emulation          |Human Resources Manager|50616          
Yamaha         |China          |1985-12-11T00:00:00.000Z|athomas2@example.com   |12             |male           |4              |200.47.210.95  |Alan Thomas     |Emulation of roots heuristic coherent systems|Resources Manager      |300000         
google         |USA            |1995-12-12T00:00:00.000Z|mbordon@example.com    |12             |male           |5              |10.47.210.65   |Michael Bordon  |Emulation of roots heuristic coherent systems|Resources Manager      |300000         
------------------------------------------------------------------------------------------------------------------------------------------------


* Delet data
PUT dtest
json data as follow:
{"percent":0.1,"stcode":"002398","time_q":"2010_6","fcode":"900089","TentNub":125.31,"netVal":1324.53}
{"percent":0.07,"stcode":"300171","time_q":"2010_6","fcode":"900089","TentNub":55.57,"netVal":899.12}

** check the index 

thie dtest is the index in es,  the data structure is stored as name dtest such as :
 curl http://10.56.233.182:9200/dtest
 {"dtest":{"aliases":{},"mappings":{"properties":{"TentNub":{"type":"float"},"fcode":{"type":"text","fields":{"keyword":{"type":"keyword","ignore_above":256}}},"netVal":{"type":"float"},"percent":{"type":"float"},"stcode":{"type":"text","fields":{"keyword":{"type":"keyword","ignore_above":256}}},"time_q":{"type":"text","fields":{"keyword":{"type":"keyword","ignore_above":256}}}}},"settings":{"index":{"creation_date":"1618389931944","number_of_shards":"1","number_of_replicas":"1","uuid":"sF5P0CNXReGZdBh_oqaC8g","version":{"created":"7030299"},"provided_name":"dtest"}}}}

** check the document
every record in this index, just two in this case

[root@master dtest]# curl -XGET http://localhost:9200/_cat/indices?v
health status index                uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   dtest                sF5P0CNXReGZdBh_oqaC8g   1   1          2            0     19.4kb          9.7kb

** check document with document id
GET cmm/_doc/rumnbXgB9UL0kp5SPUdr
{
  "_index" : "cmm",
  "_type" : "_doc",
  "_id" : "rumnbXgB9UL0kp5SPUdr",
  "_version" : 1,
  "_seq_no" : 30798580,
  "_primary_term" : 3,
  "found" : true,
  "_source" : {
    "MMEName" : "mme001",
    "CaseName" : "CMMG-9009",
    "UnitName" : "cpps0",
    "@timestamp" : "2020-10-11T16:59:20.000Z",
    "Release" : "19.5",
    "LogType" : {
      "cpcfc" : {
        "CfcTpye" : "CFCQ",
        "CFCQ" : "49-250",
        "Count" : 1
      }
    },
    "type" : "perf"
  }
}



** search all the document
in kibana console:
-----------------------
POST dtest/_search
{
    "query": {
        "match_all": {}
    }
}
---------------
in command line:
--------------------------------------------------
curl -XPOST "https://localhost:9200/_search" -d'
  {
       "query": {
              "match_all": {}
                   }
                     }'
-------------------------------

** delete document using doc id
GET cmm/_doc/rumnbXgB9UL0kp5SPUdr
DELETE cmm/_doc/rumnbXgB9UL0kp5SPUdr
curl -XDELETE http://10.56.233.182:9200/cmm/_doc/rumnbXgB9UL0kp5SPUdr


** delete both structure(mapping) and the record
*** firstly delete all the records
POST <your_index_name>/_delete_by_query
{
    "query": {
        "match_all": {}
    }
} 

*** then delete the structure
curl -XDELETE 'http://10.56.233.182:9200/dtest'
{"acknowledged":true}
#delete index:         curl -XDELETE 'localhost:9200/index_name'
#delete all indices:   curl -XDELETE 'localhost:9200/_all'
#delete document   :   curl -XDELETE 'localhost:9200/index_name/type_name/document_id'

*** stroe data directory
http://127.0.0.1:9200/_nodes/stats/fs?pretty
"path" : "C:\\ProgramData\\Elastic\\Elasticsearch\\data\\nodes\\0"

* Query
** match  
match:  The “match” query is one of the most basic and commonly used queries in Elasticsearch and functions as a full-text query. 
We can use this query to search for text, numbers or boolean values.
***  match  with single field  
syntax:
"query": { "match": { "field_name": { "query" : "field_val" } }

"query": { "match": { "phrase": { "query" : "heuristic" } }
result:
===========================================================

ts" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 0.6785374,
    "hits" : [
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.6785374,
        "_source" : { "id" : 2, "name" : "Othilia Cathel", "email" : "ocathel1@senate.gov", "gender" : "female", "ip_address" : "3.164.153.228", "date_of_birth" : "22/07/1987", 
        "company" : "Edgepulse", "position" : "Structural Engineer", "experience" : 11, "country" : "China", "phrase" : "Grass-roots heuristic help-desk", "salary" : 193530 } }, 
      { "_index" : "employees", "_type" : "_doc",
        "_id" : "4",
        "_score" : 0.6257787,
        "_source" : {
          "id" : 4,
          "name" : "Alan Thomas", "email" : "athomas2@example.com", "gender" : "male", "ip_address" : "200.47.210.95", "date_of_birth" : "11/12/1985", "company" : "Yamaha", 
               "position" : "Resources Manager", "experience" : 12, "country" : "China", "phrase" : "Emulation of roots heuristic coherent systems", "salary" : 300000 } } ] } }
++++++++++++++++++++++++++++++++++++++++++++

**** query with multiple values seprated by space,  operator default is "OR"
query without operator means OR, any of the words
{ "query": { "match": { "phrase": { "query" : "heuristic roots help"}}}}
=======================

         "phrase" : "Grass-roots heuristic help-desk",
          "salary" : 193530
         "country" : "China",
          "phrase" : "Emulation of roots heuristic coherent systems",
          "salary" : 300000
        }


**** query operator  "AND"
POST employees/_search
{ "query": { "match": { "phrase": { "query" : "heuristic roots help", "operator" : "AND" } } } }
query phrase value has three words
===========
    "hits" : [
                "id" : 2,
          "name" : "Othilia Cathel", "email" : "ocathel1@senate.gov", "gender" : "female", "ip_address" : "3.164.153.228", "date_of_birth" : "22/07/1987", "company" : "Edgepulse", "position" : "Structural Engineer", "experience" : 11,
          "country" : "China", "phrase" : "Grass-roots heuristic help-desk", "salary" : 193530
        }
=========================================

*** multi match (match with multiple fields)
POST employees/_search
{ "query": { "multi_match": { "query" : "research help" , "fields": ["position","phrase"] } } }
=================
          "position" : "Research Associate",
          "phrase" : "Multi-channelled coherent leverage",
          },
      {
         "position" : "Structural Engineer",
          "phrase" : "Grass-roots heuristic help-desk", 
====================================================

*** query with  match_phrase
match_phrase means query the exact order and words in a fields
GET employees/_search
{ "query": { "match_phrase": { "phrase": { "query": "roots heuristic coherent" } } } }
-------------------------------------------------
          "phrase" : "Emulation of roots heuristic coherent systems",
----------------------------------------

**** slop parameter
 "query": { "match_phrase": { "phrase": { "query": "roots coherent", "slop": 1 } } }
 ============================= "phrase" : "Emulation of roots heuristic coherent systems", 


*** query with match_phrase_prefix
GET employees/_search
{
"_source": [ "phrase" ],
  "query": { "match_phrase_prefix": { "phrase": { "query": "roots heuristic co" } } } }
===============
  "_source" : { "phrase" : "Emulation of roots heuristic coherent systems" }


** query with range
POST employees/_search
{ "query": { "range" : { "experience" : { "gte" : 10, "lte" : 12 } } } }
------------------------------
_source" : {
          "phrase" : "Grass-roots heuristic help-desk",
          "experience" : 11
        }
       "_source" : { "phrase" : "Versatile object-oriented emulation", "experience" : 12 }

** query with term/terms

Term level queries are used to query structured data, which would usually be the exact values.
*** query with term
"query":  { "term":  { "gender": ["female"] }}   ### only one field, multiple fields with terms instead of term
"query":  { "term":  { "experience": 12 }} }
--------
reason": "[term] query does not support array of values",


*** query with terms
"query":  { "terms":  { "gender": ["female","male"] }}


*** term(s) VS. match
String fields can be of type text (treated as full text, like the body of an email), or keyword (treated as exact values, like an email address or a zip code). 
Exact values (like numbers, dates, and keywords) have the exact value specified in the field added to the inverted index in order to make them searchable.
However, text fields are analyzed. This means that their values are first passed through an analyzer to produce a list of terms, which are then added to the inverted index.
There are many ways to analyze text: the default standard analyzer drops most punctuation, breaks up text into individual words, and lower cases them.
 For instance, the standard analyzer would turn the string “Quick Brown Fox!” into the terms [quick, brown, fox].


** query with exists
    "query": { "exists": { "field": "company" } } 


** query with sources ids (internal structure)
POST employees/_search
{ "query": { "ids" : { "values" : ["1", "4"] } } }
================================
        "_source" : { "id" : 1, "name": "hunata .ll"}
        "_source" : { "id" : 4, "name": "Alan Thomas"}

** query with prefix of fileds_value
 GET employees/_search
{ "query": { "prefix": { "name": "al" } } }
"name" : "Alan Thomas",


** query with regular expression
"query": { "regexp": { "position": "res[a-z]*" } } }
"name" : "Huntlee Dargavel",
"position" : "Research Associate"

** query with fuzzy  
  "query": { "fuzzy": { "country": { "value": "Chnia", "fuzziness": "2" } } }
 "country" : "China",


** Boolean query (Compound query)
bool could use 4 fields of clause: 
must       The clause (query) must appear in matching documents and will contribute to the score
must_not
should     results may contain this should clause or not, contain this should clause results will get more score. 
filter

The bool query takes a more-matches-is-better approach, so the score from each matching must or should clause will be added together to provide the final _score for each document.

"query": { "bool": { "must_not": [ { "exists": { "field": "AB" } } ] }

*** query with filter and multimatch
GET employees/_search
{
    "query": {
      "bool": {
        "must":{
        "multi_match" : {
            "query" : "heursitic reserch",
            "fields": ["phrase","position"],
            "fuzziness": 2
        }},
        "filter" :  { "term":{ "experience": 7 } }
    }
    },
    "size": 10
}

*** compound boolean query
(company = Yamaha OR company = Yozio ) AND (position = manager OR position = associate ) AND (salary>=100000)

****  multi_match can't suffice AND
"query": { "multi_match": { "query" : "Yamaha Yozio manager associate" , "fields": ["company", "position" ] }}
-------------------------------------
      "_source" : {
          "company" : "Talane",        ### the company is not right
          "position" : "Research Associate"
        }
      },
      {
          "company" : "Yozio",
          "position" : "Human Resources Manager"
        }
      },
      {
          "company" : "Yamaha",
          "position" : "Resources Manager"                           
------------------------------------------------

**** query with boolean
GET employees/_search
{
    "_source": [ "company", "position"],
    "query":  {
    "bool": {
       "must":[
         { "match": { "company": { "query" : "Yamaha Yozio " } }},
         { "match": { "position":  { "query" : "manager associate" } } }
        ],
       "filter" :  { "range":{ "salary": { "gte": 100000 } } }
    }
    }
}
====================================
 "hits" : [
      {
        "_source" : {
          "company" : "Yamaha",
          "position" : "Resources Manager"
        }
      },
      {
        "_source" : {
          "company" : "Yozio",
          "position" : "Human Resources Manager"
        }
      }
    ]

---------------------------------------------------------

** sort 
*** default sorting(with _score)
"_score" field: This “_score” is computed by how well the query has matched using the default scoring methodologies of Elasticsearch.
clause within filter with no "_score" coputed.
When there is no sort parameter specified in the search request, Elasticsearch returns the document based on the descending values of the “_score” field. 

*** field sort
 "query": { "match": { "phrase":{ "query": "roots" } } },
 "sort": [ { "experience": { "order": "desc" } } ]   
============
     "_source" : {
          "id" : 4, "name" : "Alan Thomas", "email" : "athomas2@example.com", "gender" : "male", "ip_address" : "200.47.210.95", "date_of_birth" : "11/12/1985", "company" : "Yamaha",
          "position" : "Resources Manager", "experience" : 12, "country" : "China", "phrase" : "Emulation of roots heuristic coherent systems", "salary" : 300000
        },
        "sort" : [ 12 ] },
      {
        "_index" : "employees", "_type" : "_doc", "_id" : "2", "_score" : null, "_source" : { "id" : 2, "name" : "Othilia Cathel", "email" : "ocathel1@senate.gov", "gender" : "female",
          "ip_address" : "3.164.153.228", "date_of_birth" : "22/07/1987", "company" : "Edgepulse", "position" : "Structural Engineer", "experience" : 11, "country" : "China",
          "phrase" : "Grass-roots heuristic help-desk", "salary" : 193530 },
        "sort" : [ 11 ] }
======================

*** multiple fields sort
"sort": [
    { "experience": { "order": "desc" } },
    { "salary": { "order": "desc" } }
 ] 
sort will based on experience firstly, if experience is the same then sort based on salary


** Boosting Queries
there are requirements in the search criteria where we neeed to demote certain search results but don not want to 
omit them for the search results altogether.
-------------------------------------------
POST  employees/_search
{
    "query": {
    "boosting" : {
            "positive" : { "match": { "country": "china" } },
            "negative" : { "match": { "company": "Talane" } },
            "negative_boost" : 0.5
        } } } 
----------------
     {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "4",
        "_score" : 0.105360515,
        "_source" : {
          "country" : "China",
          "name" : "Alan Thomas",
          "company" : "Yamaha"
        }
      },
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.052680258,    ### score is 0.5 less than previous one, since Talane is in negative clause
        "_source" : {
          "country" : "China",
          "name" : "Huntlee Dargavel",
          "company" : "Talane"
        }
      }
    ]
===============================================

*** multiple boosting query with boolean
GET employees/_search
{
  "query": {
    "boosting": {
      "positive": { "bool":
                          { "should": [ { "match": { "country": { "query": "usa" } } },
                                        { "range": { "experience": { "gte": 10 } } } ]
                  } },
      "negative": { "match": { "gender": "female" } },
      "negative_boost": 0.5
    } } }

** query string
POST employees/_search
{
  "query": {
    "query_string": {
      "query": "(roots heuristic systems) OR (enigneer~) OR (salary:(>=10000 AND <=52000)) ",
      "fields": [
        "position",
        "phrase^3"
      ]
    }
  }

** Function Score Queries

The function_score query enables us to change the score of the documents that are returned by a query. The function_score
 query requires a query and one or more functions to compute the score.
score function could be used to contribute to calculate the _score with some mode.
***  functions Syntax using filter and weight
GET employees/_search
{
"_source": ["position","phrase"], 
  "query": {
    "function_score": { "query": { "match": { "position": "manager" } }, # query score is es's own algorithm
      "functions": [
        { "filter": { "match": { "phrase": "coherent" } },    #id4_or_score ,id5_or_score  *2
          "weight": 2
        },
        { "filter": { "match": { "phrase": "emulation" } },  #id4_or_score ,id5_or_score, id3_or_score  *10 
          "weight": 10
        }
      ],
      "score_mode": "multiply",   ####this means function's weight * es'own score will make final _scor *2 or *10 
      "boost": "5",              ##### all match id_score * 5
      "boost_mode": "multiply"
    }
  }
}
----------------------
  "hits" : [
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "4",
        "_score" : 55.981613,
        "_source" : {
          "phrase" : "Emulation of roots heuristic coherent systems",
          "position" : "Resources Manager"
        }
      },
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "5",
        "_score" : 55.981613,
        "_source" : {
          "phrase" : "Emulation of roots heuristic coherent systems",
          "position" : "Resources Manager"
        }
      },
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 23.459919,
        "_source" : {
          "phrase" : "Versatile object-oriented emulation",
          "position" : "Human Resources Manager"
        } } ] } }
====================================
**** orginal score
"hits" : [
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "4",
        "_score" : 0.5598161,
        "_source" : {
          "phrase" : "Emulation of roots heuristic coherent systems",
          "position" : "Resources Manager"
        }
      },
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "5",
        "_score" : 0.5598161,
        "_source" : {
          "phrase" : "Emulation of roots heuristic coherent systems",
          "position" : "Resources Manager"
        }
      },
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "3",
        "_score" : 0.46919838,
        "_source" : {
          "phrase" : "Versatile object-oriented emulation",
          "position" : "Human Resources Manager"
        }
      }
    ]
  }
}
**** score_mode
score_mode specifies how the computed scores are combined:

multiply     scores are multiplied (default)
sum          scores are summed
avg          scores are averaged 
max          maximum score is used
min          minimum score is used 
first        the first function that has a matching filter is applied


**** boost_mode
multiply     scores are multiplied (default)
sum          scores are summed
avg          scores are averaged 
max          maximum score is used
min          minimum score is used 
replace      override the es's own query's score with boots's score

*** function with script in query
------------
GET employees/_search
{
  "_source": [
    "name",
    "experience",
    "salary"
  ],
  "query": {
    "function_score": {
      "query": {
        "match_all": { }  #### get all the records, 
      },
      "functions": [
        {
          "script_score": {
            "script": {
              "source": "(doc['salary'].value/doc['experience'].value)/1000"
            } } } ],
     "boost_mode": "replace"  ## use scripts' calculation result as final score no related to orginal es query score
    } } }
----------------------
"hits" : [
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 25.0,  #### 180025/7/1000 = 25
        "_source" : {
          "name" : "Huntlee Dargavel",
          "experience" : 7,
          "salary" : 180025
        }
      },
###########

*** 
*** function_score with field_value_factor

We can make use of a field from the document to influence the score by using the “field_value_factor” function. This is in some ways a simple alternative to “script_score”. In our example, let us make use of the “experience” field value to influence our score as below

GET employees/_search
{
  "_source": ["name","experience"], 
    "query": {
        "function_score": {
            "field_value_factor": {
                "field": "experience", ### _score= square( experence.vale * 0.5 )       
                 "factor": 0.5,
                "modifier": "square",
                "missing": 1
            }
        }
    }
}

--------------
       "_score" : 36.0,  square (12 *0.5) =36
        "_source" : { "name" : "Winston Waren",
          "experience" : 12
-----------------------------

*** function_score: Decay Functions
### salary is baseline is 200000, and scale is +-30000, that salary is (170000,230000) will get highest score, and make
## out of range salary very low score
GET employees/_search
{
  "_source": [
    "name",
    "salary"
  ],
  "query": {
    "function_score": {
      "query": {
        "match_all": {}
      },
      "functions": [
       {
         "gauss": {
           "salary": {
             "origin": 200000,
             "scale": 30000
           }
         }
       }
      ],
      "boost_mode": "replace"
    }
  }
}
----------------------------------
   {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "1",
        "_score" : 0.7354331,    #### this is the normal es query score which is in range
        "_source" : {
          "name" : "Huntlee Dargavel",
          "salary" : 180025
        } },
      {
        "_index" : "employees",
        "_type" : "_doc",
        "_id" : "4",
        "_score" : 4.5208726E-4,    ####out of range score is much less than in range score, it will be decayed very obviously
        "_source" : {
          "name" : "Alan Thomas",
          "salary" : 300000
        } },
----------------------------------

* _msearch Query
normally we use _search to execute one clause in es.
on order to execute multiple clause in es, _msearch used instead.
=====================
GET cmm/_msearch
{"index": "cmm"}
{"query": {"match_all": {}}}
===============================
note every clause must withine one line, it can't seprated as many lines.
the first clause means which index to search, the second should be the query clause.


===========
GET my-index-000001/_msearch
{ }      ### this means use my-index-000001 as index
{"query" : {"match" : { "message": "this is a test"}}}   ### search message with "this is a test" in index my-index-000001
{"index": "my-index-000002"}       #### use my-index-000002 as index
{"query" : {"match_all" : {}}}     #### search all in index my-index-000002
=============================



* parent document
===================================
"_source" : {
          "document_type" : {
            "name" : "post"
          },
          "post_title" : "Angel Has Fallen"
        }
      },
      {
        "_index" : "post-comments",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "document_type" : {
            "name" : "post"
          },
          "post_title" : "Beauty and the beast - a nice movie"
        }


* Aggregations
an aggregation summarize your data as metrics, statistics, or other analytics.

three categories of aggregation
Metric : aggregations that calculate metircs, such as a sum or average from fied values.
Bucket : aggregations that group documents into buckets, also called bins based on filed values, ranges, or other criteria
Pipeline: aggregations that take input from other aggregations instead of documents or fields


** Metrics aggregations
    Avg
    Max
    Min
    Sum
    Rate
    Top metrics
    Percentiles
nly:
    Scripted metric
    Value count
    Weighted avg





** bucket aggregation

*** terms agrregation
"terms": {  "field":  <fieldname>}
A multi-bucket value source based aggregation where buckets are dynamically built - one per unique value.
console input
=============
GET _search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "FailCount"
      }
    }
  }}
===============

output
========================
"aggregations" : {
    "my-agg-name" : {
      "doc_count_error_upper_bound" : 3181,
      "sum_other_doc_count" : 168806,
      "buckets" : [
        {
          "key" : 0,
          "doc_count" : 471039
        },
        {
          "key" : 1,
          "doc_count" : 86119
        },
        {
          "key" : 2,
          "doc_count" : 2339
        },

==================================

*** Size

The size parameter can be set to define how many term buckets should be returned out of the overall terms list. By default, the node coordinating the search process will request each shard to provide its own top size term buckets and once all shards respond, it will reduce the results to the final list that will then be returned to the client. This means that if the number of unique terms is greater than size, the returned list is slightly off and not accurate
console input
=============
GET cmm/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "FailCount",
        "size": 2
      }
    }
  }}
===============

output
========================
"aggregations" : {
    "my-agg-name" : {
      "doc_count_error_upper_bound" : 3181,
      "sum_other_doc_count" : 168806,
      "buckets" : [
        {
          "key" : 0,
          "doc_count" : 471039
        },
        {
          "key" : 1,
          "doc_count" : 86119
        },
==================================


***  nested aggs
"aggs":{
        "L1":{
            "date_histogram":{
                "field":"@timestamp",   ####L1 aggs bucket key will be timestamp and next to one is 1h plus
                "fixed_interval":"1h",
                "time_zone":"UTC",
                "min_doc_count":1
            },
             "aggs":{  ### 1h of teimestamp in bucket L1,
                "FailCount":{
                    "terms":{
                        "field":"CFC",   ####bucket key is CFC's different value
                        "order":{ "avg_field_failcount":"desc" }, ## descendant order, top avg_field_faicount
                        "size":5                ## 5 top avg_feield_failcount value will be selected in the group of CFC
                    },
                    "aggs":{
                        "avg_field_failcount":{
                            "avg":{
                                "field":"FailCount"   #### bucket key will be avg value of FailCount
                            }
                        }
                    }
                }
      }}

====================
"aggregations" : {
    "L1" : {
      "buckets" : [
        {
          "key_as_string" : "2020-10-11T14:00:00.000Z",
          "key" : 1602424800000,
          "doc_count" : 225,
          "FailCount" : {
            "doc_count_error_upper_bound" : -1,
            "sum_other_doc_count" : 183,
            "buckets" : [
              {
                "key" : 15,      ####CFC value
                "doc_count" : 6,
                "avg_field_failcount" : {
                  "value" : 129.0      ##### top1
                }
              },
              {
                "key" : 43,
                "doc_count" : 6,
                "avg_field_failcount" : {
                  "value" : 54.0      #### top2
                }
              },
===================================

***  different cfc code as bucket1, then every 10s, then calculate the percentage 
------------------------
"aggs": {
    "lEV1": {
      "terms": {
        "field" : "CFC", 
        "size" : 100},
    
      "aggs":{
       "dat_aggs": {
        "date_histogram": {
          "field": "@timestamp",
          "fixed_interval": "10s",
          "min_doc_count": 1
        },
        "aggs":{
             "l2":  {
               "terms":{
      "script": "doc['SucCount'].value * 10000/ (doc['FailCount'].value + doc['SucCount'].value) "
               }
             }
                    }
-------------------
result:
+++++++++++
"aggregations" : {
    "lEV1" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [
        {
          "key" : 1,   ##### CFC code 1
          "doc_count" : 9,   #### 9 10s interval timestamp
          "dat_aggs" : {
            "buckets" : [
              {
                "key_as_string" : "2020-10-11T14:00:10.000Z",
                "key" : 1602424810000,
                "doc_count" : 1,   ### only 1 failcount since it's the smallest sample interval
                "l2" : {
                  "doc_count_error_upper_bound" : 0,
                  "sum_other_doc_count" : 0,
                  "buckets" : [
                    {
                      "key" : "7702",    ##### successful rate 77.02%
                      "doc_count" : 1
                    }
                  ]
                }
              },
              {
                "key_as_string" : "2020-10-11T14:00:20.000Z",
                "key" : 1602424820000,
                "doc_count" : 1,
                "l2" : {
                  "doc_count_error_upper_bound" : 0,
                  "sum_other_doc_count" : 0,
                  "buckets" : [
                    {
                      "key" : "7692",
                      "doc_count" : 1
                    }
                  ]
                }
              },
+++++++++++++++++++++++++++++++++++++++++++++

*** pipeline aggregation
the pipeline use parent aggs to generate a new value
**** cumulative_sum
======================================
POST /sales/_search
{
  "size": 0,
  "aggs": {
    "sales_per_month": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "month"
      },
      "aggs": {
        "sales": {
          "sum": {
            "field": "price"
          }
        },
        "cumulative_sales": {  
          "cumulative_sum": {
            "buckets_path": "sales" #### paraent aggs sales as bucket path to calculate the cumulative_sum for every next one
    } } } } } }
=======================================================
result:
++++++++++++
 "aggregations": {
      "sales_per_month": {
         "buckets": [
            {
               "key_as_string": "2015/01/01 00:00:00",
               "doc_count": 3,
               "sales": { "value": 550.0 },
               "cumulative_sales": { "value": 550.0 }
            },
            {
               "key_as_string": "2015/02/01 00:00:00",
               "doc_count": 2,
                "sales": { "value": 60.0 },
               "cumulative_sales": { "value": 610.0 }
            },
            { "key_as_string": "2015/03/01 00:00:00",
               "doc_count": 2,
               "sales": { "value": 375.0 },
               "cumulative_sales": { "value": 985.0 }
+++++++++++++++++++++


**** bucket_sort using bucket name in aggregation
Syntax:
"<bucket_sort_name>" :{
  "bucket_sort": {
    "sort": [
      { "sort_field_1": { "order": "asc" } },   
      { "sort_field_2": { "order": "desc" } },
      "sort_field_3"
    ],
    "from": 1, ### the basic result from index 1
    "size": 3  ## total results
  }
}
----------
POST /sales/_search
{
  "size": 0,
  "aggs": {
    "sales_per_month": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "month"
      },
      "aggs": {
        "total_sales": {
          "sum": {
            "field": "price"
          }
        },
        "sales_bucket_sort": {
          "bucket_sort": {
            "sort": [
              { "total_sales": { "order": "desc" } }  ### total_sales is the aggregation bucket name 
            ],
            "size": 3                                
          } } } } } }
----------------------

#### all total_sales in decendant value, top 3 monthly sales
{
   "aggregations": {
      "sales_per_month": {
         "buckets": [
            {
               "key_as_string": "2015/01/01 00:00:00",
               "key": 1420070400000,
               "doc_count": 3,
               "total_sales": {
                   "value": 550.0
               }
            },
            {
               "key_as_string": "2015/03/01 00:00:00",
               "key": 1425168000000,
               "doc_count": 2,
               "total_sales": {
                   "value": 375.0
               },
            },
            {
               "key_as_string": "2015/02/01 00:00:00",
               "key": 1422748800000,
               "doc_count": 2,
               "total_sales": {
                   "value": 60.0
               },
            }
         ]
      }
   }
}
-----------------------
**** bucket_script
Syntax:
"<bucket_script_name>" :{
  "bucket_script": {
      "buckets_path": {
#           "<params name in scriipt>":  "<aggreation bucket name>"
              "gain":  "sum_gain",
               "year": "sum_year"
              },
      "script": "params.gain/params.year "
                   }
               },

  


*** bucket_sort with pipeline name(generated from the bucket_script)
#####################
 
"aggs": {
     "fundmanager": {
      "terms": { "field": "fd_manager.keyword",
      "size":   26324   #### here the size is important, or it will not be the total data
      },
      "aggs": {
           "ocpu_mang": { "avg": { "field": "fd_man_time" } },
           "sum_gain": { "sum": { "field": "gainMargin" } },
           "sum_year": { "sum": { "field": "f_totaltime"  } },
           "year-percentage": {
                     "bucket_script": {
                        "buckets_path": {
                          "gain":  "sum_gain",
                           "year": "sum_year"
                     },
                    "script": "params.gain/params.year "
                   }
               },
           "sales_bucket_sort": {
              "bucket_sort": {
            "sort": [
              { "year-percentage": { "order": "desc" } }  ### bucket_sort using pipeline name in the aggregation
            ],
            "size": 20  ### get top 20 of the year-percentage
          } }
         }


*** nested level 2 aggs field  
  "aggs": {
        "sales_per_month":{
            "date_histogram":{ "field":"@timestamp", "fixed_interval":"10s", "time_zone":"UTC", "min_doc_count":1 },
            "aggs": {
             "Procedure": { "terms":{ "field":"LogType.cpMaf.Procedure.keyword", "size":4 } },
             "total_fails": { "sum": { "field": "LogType.cpMaf.Fail" } },
             "totals": { "sum": { "field": "LogType.cpMaf.Total" } },
             "fail-percentage": {
                 "bucket_script": {
                    "buckets_path": {
                      "fail":  "total_fails",
                       "total": "totals"
                 },
                "script": "params.fail/params.total * 100"
               }
           }
           } }   
================================
"aggregations" : {
    "sales_per_month" : {
      "buckets" : [
        {
          "key_as_string" : "2020-10-11T14:00:00.000Z",
          "key" : 1602424800000,
          "doc_count" : 38,
          "Procedure" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 34,
            "buckets" : [
              {
                "key" : "Attach",
                "doc_count" : 1
              },
              {
                "key" : "Cancel Location",
                "doc_count" : 1
              },
              {
                "key" : "Context Release",
                "doc_count" : 1
              },
              {
                "key" : "Context Request",
                "doc_count" : 1
              }
            ]
          },
          "totals" : {
            "value" : 23607.0
          },
          "total_fails" : {
            "value" : 292.0
          },
          "fail-percentage" : {
            "value" : 1.2369212521709663
          }
        },

*** nested pipeline aggregation and sorting
pipeline generation then sort using pipeline
pipeline generation of fail-percentage which is a scrpit calculated two fields total and fail 

"aggs": {
        "sales_per_month_L1":{
            "date_histogram":{ "field":"@timestamp", "fixed_interval":"10s", "time_zone":"UTC", "min_doc_count":1 },
             "aggs": {
              "Procedure_L2": {
                 "terms":{
                        "field":"LogType.cpMaf.Procedure.keyword",
                        "size":3
                  },
                "aggs": { 
                 "Maf_total_L3":   { "avg": {     "field": "LogType.cpMaf.Total"    } },
                 "Maf_fail_L3": { "avg": {      "field": "LogType.cpMaf.Fail"   }},
                 "fail-percentage_L3": {
                       "bucket_script": {
                         "buckets_path": {
                          "fail":  "Maf_fail_L3",
                           "total": "Maf_total_L3"
                 },
                  "script": "params.fail/params.total * 100"
                 }
                },
                "final_sort": {  "bucket_sort": { "sort": [ {"fail-percentage": {"order": "desc"}}      ] } }
               #### bucket_sort with pipeline "fail-percentage"
              }            
            }
              
           }  
        }   
   },
###########################
    {
       
          "LogType" : {
            "cpMaf" : {
              "PerSec" : 0,
              "Total" : 4,     ##LogType.cpMaf.Total
              "Success" : 3,
              "Percent" : 0.0,
              "Fail" : 1,      ##LogType.cpMaf.Fail
              "Procedure" : "SGS Page Request CS STMSI" ###LogType.cpMaf.Procedure.keyword
            }
          }
        },
        "fields" : {
          "@timestamp" : [
            "2020-10-11T14:00:00.000Z"
          ]
        }
      },

--------------------------
       {
          "key_as_string" : "2020-10-11T14:00:20.000Z",
          "key" : 1602424820000,
          "doc_count" : 36,
          "Procedure" : {
            "doc_count_error_upper_bound" : 0,
            "sum_other_doc_count" : 33,
            "buckets" : [ { "key" : "Attach", "doc_count" : 1, "L3" : { "value" : 78.0 }, "L3_p" : { "value" : 18.0 },
                "fail-percentage" : { "value" : 23.076923076923077 } },
              { "key" : "Cancel Location", "doc_count" : 1, "L3" : { "value" : 162.0 }, "L3_p" : { "value" : 0.0 }, "fail-percentage" : { "value" : 0.0 } },
              { "key" : "Context Release", "doc_count" : 1, "L3" : { "value" : 8924.0 }, "L3_p" : { "value" : 0.0 }, "fail-percentage" : { "value" : 0.0 } }
            ]
          }

*** nested aggregation search  
POST /sales/_search
{
    "size": 0,
    "aggs" : {
        "sales_per_month" : {
            "date_histogram" : { "field" : "date", "interval" : "month" }, ##L1 bucket based on date per month
            "aggs": {
                "total_sales": { "sum": { "field": "price" } }, ####aggs L2 total_sales
                "t-shirts": {                                   ####aggs L2 t-shirts 
                     "filter": { "term": { "type": "t-shirt" } },
                     "aggs": { "sales": { "sum": { "field": "price" } } }
                 },
                "t-shirt-percentage": {                       #####aggs L2 t-shirt-percentage
                    "bucket_script": {
                        "buckets_path": {
                          "tShirtSales": "t-shirts>sales",
                          "totalSales": "total_sales"
                        },
                        "script": "params.tShirtSales / params.totalSales * 100"
                    }
                }
            }
        } } }

*** count the aggregation doc_count in bucketes
GET /cars/transactions/_search
{
    "size" : 0,
    "aggs" : {
        "colors" : {
            "terms" : {
              "field" : "color",
              "order": {
                "_count" : "asc" 
              }
            }
        }
    }
}

*** count a filed number
"aggs" : {
        "types_count" : { "value_count" :
         { "field" : "fcode.keyword" } }
    } 


*** state the aggregation's nested aggreation
keyword is "stats_bucket"
"aggs": {
    "stockL1": {
      "terms": {
        "field": "stcode.keyword",
         "order":{ "num_L2":"asc" },
        "size": 1126269
      },
      "aggs":{
        "num_L2":{
           "sum":{ "field":"TentNub" }
        }
      }
    },
    "sum_stat_buck": {
      "stats_bucket": {
        "buckets_path": "stockL1>num_L2" 
      }
    }
  }
===================================
......
     {
          "key" : "000725",
          "doc_count" : 1301,
          "num_L2" : {
            "value" : 590455.9006035272
          }
        }
      ]
    },
    "sum_stat_buck" : {
      "count" : 4313,
      "min" : 0.0,
      "max" : 590455.9006035272,
      "avg" : 5143.283038988998,
      "sum" : 2.2182979747159548E7
    }
  }
}

* kibana consle VS. curl
  all operation to es(POST DELETE GET) could be passed to es via kibana console or curl

** in kibana console:
-----------------------
POST dtest/_search
{ "query": { "match_all": {} } } 
----------------------------
DELETE cmm/_doc/rumnbXgB9UL0kp5SPUdr


** curl command line:
--------------------------------------------------
curl -XPOST "http://localhost:9200/_search" -d'
  { "query": { "match_all": {} } }'
-------------------------------

curl -XDELETE http://10.56.233.182:9200/cmm/_doc/rumnbXgB9UL0kp5SPUdr


*** curl exectute complicate clause using file
 curl -XGET  http://10.56.233.182:9200/dtest/_search -H "Content-Type: application/json" -d @js_f.txt

js_f.txt stored the query clause
-------
[root@master fdds]# cat js_f.txt
{
  "query": {
      "exists":{
               "field": "LogType"
        }
  }
}
----------------------------

*  es search results field
  "took" : 2,               ## how much time(in seconds) did the search take 
  "timed_out" : false,      ## waiting for result timeout or not, should be false, or the result is no accurate
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },

** shard 
Each shard is an instance of Lucene index, which you can think of as a self-contianed search
engine that indexes and handles queries for a subset of the data in an Elasticsearch cluster


** "track_total_hits": true option
GET dtest/_search
{
  "size": 0,
    "track_total_hits": true,
  "query": { "bool": { "must":[ { "exists": { "field": "time_q" } } ] } } } 
-----------------------------------------------
"hits" : {
    "total" : { "value" : 492176, "relation" : "eq" },
####gets all the document number is 492176


**** "track_total_hits" default is false
in default the hits value is 10000
  "hits" : {
    "total" : { "value" : 10000, "relation" : "gte" },


GET dtest/_search
{
  "size": 0,
    "track_total_hits": true,
  "query": {
    "bool": { "must":[ { "match": { "time_q": { "query" : "2010_12" }}} ] } }
}
===============================================
  "hits" : {
    "total" : {
      "value" : 19061
      "relation" : "gte"
    }

* logstash and elasticsearch
su fund -c bin/elasticsearch > nohup.out 2>&1
** logstash configuration
*** logstash startup command
bin/logstash --verbose -f dtoest
-- path.data <dir/path>  ### this is optinal

***  logstash configuration directory
these input  filter output section could be in different files within the configration directory or all-in-one file dd.conf

[root@master fdds]# cat dtest/dd.conf
============================================
input {
  file {
    path => "/root/Downloads/fdds/logstash_work/input/*"
    start_position => "beginning"
    sincedb_path => "/root/Downloads/fdds/logstash_work/sin/*"
    sincedb_write_interval => 15
    codec => json {charset => "UTF-8"}
  }
}

filter {
     mutate {
        remove_field => ["@version", "message", "path", "host", "@timestamp"]
      }
}

output {
    stdout {}
     elasticsearch {
      hosts => ["10.56.233.182:9200"]
      index => "dtest"
    }
}

======================================================

logstash will monitor the input->file->path, if there's any change to it, logstash will synchronize content to elasticsearch

**** json file as input
[root@master fdds]# head /root/Downloads/fdds/logstash_work/input/fin_res_3_50
{"key":"600519", "doc_count":1679, "stValue_L2":7505325.098679543, "holdst_L2":6756.859968742356,  "rank":1,  "time_qt":"2020_3"}
{"key":"601318", "doc_count":1158, "stValue_L2":5182594.7011060715, "holdst_L2":74888.14988593198,  "rank":2,  "time_qt":"2020_3"}
{"key":"600276", "doc_count":1182, "stValue_L2":3741176.2489744425, "holdst_L2":40651.78003027104,  "rank":3,  "time_qt":"2020_3"}
{"key":"000858", "doc_count":742, "stValue_L2":3375430.7229230404, "holdst_L2":29299.709926519543,  "rank":4,  "time_qt":"2020_3"}


**** netsted fields in es
{"timestamp": "2020-10-30 03:22:38", "olc": {"Total_Ltnc": 0, "Total_HighW": 0, "Total_Rcv": 2159, "Total_RspSucc": 2158, "Total_RspFail": 0, "Total_Discard": 0}, "CaseName": "CMMG-9007", "MMEName": "mme001", "Release": "20.0", "UnitName": "dbs3"}
{"timestamp": "2020-10-30 03:22:38", "olc": {"DB": 0, "RdReq": 1073, "RdRsp": 0, "WrReq": 1028, "WrRsp": 0, "DelReq": 1, "GenCmd": 1}, "CaseName": "CMMG-9007", "MMEName": "mme001", "Release": "20.0", "UnitName": "dbs3"}
{"timestamp": "2020-10-30 03:22:38", "olc": {"DB": 1, "RdReq": 1086, "RdRsp": 0, "WrReq": 1023, "WrRsp": 0, "DelReq": 6, "GenCmd": 1}, "CaseName": "CMMG-9007", "MMEName": "mme001", "Release": "20.0", "UnitName": "dbs3"}
{"timestamp": "2020-10-30 03:22:38", "olc": {"Sum_RdReq": 2159, "Sum_RdRsp": 0, "Sum_WrReq": 2051, "Sum_WrRsp": 0, "Sum_DelReq": 7, "Sum_GenCmd": 2}, "CaseName": "CMMG-9007", "MMEName": "mme001", "Release": "20.0", "UnitName": "dbs3"}

here field "olc" or field "olc.Total_Ltnc" could be refer to
POST cmm/_search
{ "query": { "exists":{ "field": "olc" } } }
------------
{
        "_index" : "cmm",
        "_type" : "_doc",
        "_id" : "fMN2-HgB6I4yarDqwbov",
        "_score" : 1.0,
        "_source" : {
          "type" : "perf",
          "@timestamp" : "2020-10-30T03:22:16.000Z",
          "Release" : "20.0",
          "MMEName" : "mme001",
          "olc" : {
            "HighW" : 4371,
            "Proc" : 213,
            "Rcv" : 48,
            "RspSucc" : 48,
            "Discard" : 0,
            "Ltnc" : 4,
            "RspFail" : 0
          },
          "UnitName" : "dbs3",
          "CaseName" : "CMMG-9004"
        }
      },
      {
        "_index" : "cmm",
        "_type" : "_doc",
        "_id" : "tcN2-HgB6I4yarDqwbgt",
        "_score" : 1.0,
        "_source" : {
          "type" : "perf",
          "@timestamp" : "2020-10-30T03:22:39.000Z",
          "Release" : "20.0",
          "MMEName" : "mme001",
          "olc" : {
            "HighW" : 314,
            "Proc" : 214,
            "Rcv" : 24,
            "RspSucc" : 24,
            "Discard" : 0,
            "Ltnc" : 7,
            "RspFail" : 0
          },
          "UnitName" : "dbs3",
          "CaseName" : "CMMG-9004"
        }
      },
-------------------------------
or 
{ "query": { "exists":{ "field": "olc.HighW" } } }

